{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bKLiClRmDpPY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3604,
     "status": "ok",
     "timestamp": 1699871513972,
     "user": {
      "displayName": "inês Sampaio",
      "userId": "15878877488891250619"
     },
     "user_tz": -60
    },
    "id": "C1tpx4Hug427",
    "outputId": "5da20359-c71e-4bb4-9220-86cccf50ea75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "import sys\n",
    "sys.path.insert(0,'****/Functions_Classes')\n",
    "import confounder_correction_classes_combat_modmean_lr_normal\n",
    "from confounder_correction_classes_combat_modmean_lr_normal import ComBatHarmonization,StandardScalerDict, BiocovariatesRegression\n",
    "\n",
    "#Datasets and system\n",
    "import numpy as np                # Management of arrays\n",
    "import os                         # System utils\n",
    "import pandas as pd\n",
    "from pathlib import Path          # path and file utils\n",
    "from scipy.io import loadmat      # Required to load .mat files\n",
    "import h5py #For creating new dataset files\n",
    "import sklearn\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q5lH3OQXInQD"
   },
   "source": [
    "# Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vZJWgr8J33U8"
   },
   "outputs": [],
   "source": [
    "# Atlas regions names\n",
    "data_path='****/Organized_Inputs/'\n",
    "\n",
    "atlas_ROI_names=pd.read_csv(os.path.join(data_path, \"atlas_ROI_sorted.csv\"))\n",
    "\n",
    "# HCP\n",
    "HCP_covars_raw=pd.read_csv(os.path.join(data_path, \"HCP_covar.csv\"))\n",
    "HCP_data_raw=pd.read_csv(os.path.join(data_path, \"HCP_data.csv\"))\n",
    "n_HCP=HCP_data_raw.shape[0]\n",
    "\n",
    "# test set\n",
    "Strat_covars_raw=pd.read_csv(os.path.join(data_path, \"test_set_YA_covar_raw.csv\"))\n",
    "Strat_data_raw=pd.read_csv(os.path.join(data_path, \"test_set_YA_data_raw.csv\"))\n",
    "\n",
    "n_Strat=Strat_data_raw.shape[0]\n",
    "\n",
    "# Globals\n",
    "\n",
    "strat_globals_data=copy.deepcopy(Strat_covars_raw[['TIV','CT_avg','CT_std','abGM','abWM','abCSF']])\n",
    "strat_covars=copy.deepcopy(Strat_covars_raw[['Batch','Age','Gender','Diagnosis','index']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lOb5OX1o5fXf"
   },
   "outputs": [],
   "source": [
    "\n",
    "HCP_covars_raw[['Batch']]=0\n",
    "HCP_covars_raw[['Diagnosis']]=0\n",
    "\n",
    "column_to_move = HCP_covars_raw.pop(\"Batch\")\n",
    "# insert column with insert(location, column_name, column_value)\n",
    "HCP_covars_raw.insert(0, \"Batch\", column_to_move)\n",
    "column_to_move = HCP_covars_raw.pop(\"Diagnosis\")\n",
    "# insert column with insert(location, column_name, column_value)\n",
    "HCP_covars_raw.insert(3, \"Diagnosis\", column_to_move)\n",
    "HCP_covars_raw\n",
    "\n",
    "# Globals\n",
    "hcp_globals_data=HCP_covars_raw[['TIV','CT_avg','CT_std','abGM','abWM','abCSF']].copy()\n",
    "HCP_covars=HCP_covars_raw[['Batch','Age','Gender', 'Diagnosis']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lDDN-g1S5yOy"
   },
   "outputs": [],
   "source": [
    "HCP_data_raw=HCP_data_raw.iloc[:,0:170].copy()\n",
    "Strat_data_raw=Strat_data_raw.iloc[:,0:170].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SZoi0UanD3TD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wnt3VnBcc-i9"
   },
   "source": [
    "# CR pipeline TIV: M-ComBat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 227,
     "status": "ok",
     "timestamp": 1698432336671,
     "user": {
      "displayName": "inês Sampaio",
      "userId": "15878877488891250619"
     },
     "user_tz": -120
    },
    "id": "FHgRdDqzc-i-",
    "outputId": "7066e4c5-f658-4b41-9a4a-72223c37fb4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7]\n",
      "[neuroCombat] Creating design matrix\n",
      "[neuroCombat] Standardizing data across features\n",
      "[neuroCombat] Fitting L/S model and finding priors\n",
      "[neuroCombat] Finding parametric adjustments\n",
      "[neuroCombat] Final adjustment of data\n",
      "Batchs for this set ['0' '1' '2' '3' '4' '5' '6' '7']\n",
      "Batchs from the training set estimation ['0' '1' '2' '3' '4' '5' '6' '7']\n",
      "M-ComBat option was used during fitting\n",
      "Batches in current set matching the ones from training set estimation [0 1 2 3 4 5 6 7]\n",
      "Transforming tthe reference and other sets used in the combat estimation\n",
      " Reintroducing original data for all data from reference site\n",
      "Batchs for this set ['1' '2' '3' '4' '5' '6' '7']\n",
      "Batchs from the training set estimation ['0' '1' '2' '3' '4' '5' '6' '7']\n",
      "M-ComBat option was used during fitting\n",
      "Batches in current set matching the ones from training set estimation [1 2 3 4 5 6 7]\n",
      "Transforming a test set with reference estimations\n"
     ]
    }
   ],
   "source": [
    "# M-ComBat Variation: Data is harmonized to a reference batch, in this case the HCP HC dataset\n",
    "ref_batch=0 # Is the HCP dataset\n",
    "\n",
    "globals_id=np.arange(0,6)\n",
    "\n",
    "# The TIV is harmonized together with globals\n",
    "globals_of_interest={'globals': {'id': globals_id, 'categorical': ['Gender'],\n",
    "                               'continuous':['Age']} }\n",
    "\n",
    "# Multi-center TIV Harmonization\n",
    "combat_ref_TIV=ComBatHarmonization(cv_method=None, ref_batch=ref_batch,\n",
    "                                   regression_fit=0,feat_detail=globals_of_interest,\n",
    "                                   feat_of_no_interest=None)\n",
    "\n",
    "# Concatenate Strat HC and HCP bootstrap sample\n",
    "\n",
    "# Concatenate all HC data together\n",
    "HC_data=pd.concat([HCP_data_raw, Strat_data_hc],axis=0).reset_index(drop=True)\n",
    "HC_covars=pd.concat([HCP_covars,strat_covars_hc],axis=0).reset_index(drop=True)\n",
    "HC_covars['Batch']=HC_covars['Batch'].astype('int')\n",
    "HC_covars.rename(columns={\"Batch\":\"batch\"}, inplace=True)\n",
    "HC_globals=pd.concat([hcp_globals_data,strat_globals_hc],axis=0).reset_index(drop=True)\n",
    "\n",
    "# Harmonized TIV\n",
    "\n",
    "# Estimate combat center parameters for HC test set\n",
    "HC_globals_dict={'data': HC_globals, 'covariates': HC_covars}\n",
    "\n",
    "HC_globals_harm=combat_ref_TIV.fit_transform(HC_globals_dict)\n",
    "\n",
    "# Apply combat center parameters to not_HC test set\n",
    "strat_covars_nothc.rename(columns={\"Batch\":\"batch\"}, inplace=True)\n",
    "\n",
    "notHC_globals_dict={'data': strat_globals_nothc, 'covariates': strat_covars_nothc}\n",
    "\n",
    "notHC_globals_harm=combat_ref_TIV.transform(notHC_globals_dict)\n",
    "\n",
    "# Extrat harmonized TIV\n",
    "hc_harm_TIV=pd.DataFrame(HC_globals_harm[:,0], columns=['TIV'])\n",
    "nothc_harm_TIV=pd.DataFrame(notHC_globals_harm[:,0], columns=['TIV'])\n",
    "\n",
    "# Replace raw TIV covariate with harmonized TIV\n",
    "HC_covars['TIV']=hc_harm_TIV\n",
    "strat_covars_nothc['TIV']=nothc_harm_TIV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ynx9yjOLc-i_"
   },
   "outputs": [],
   "source": [
    "HCP_globals_harm=pd.DataFrame(HC_globals_harm[:1109], columns=['TIV','CT_avg','CT_std','abGM','abWM','abCSF'])\n",
    "Strat_globals_harm=pd.concat([pd.DataFrame(HC_globals_harm[1109:]), pd.DataFrame(notHC_globals_harm)], axis=0)\n",
    "Strat_globals_harm.columns=['TIV','CT_avg','CT_std','abGM','abWM','abCSF']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-oxWEXHYc-i_"
   },
   "outputs": [],
   "source": [
    "main_path='****/Corrected_withglobals_modmean/combat_modmean_lr_betas/'\n",
    "\n",
    "path= main_path + 'test_set_YA_globals_harm.csv'\n",
    "with open(path, 'w', encoding = 'utf-8-sig') as f:\n",
    "  pd.DataFrame(Strat_globals_harm).to_csv(f,index=False)\n",
    "\n",
    "path= main_path + 'HCP_globals_harm.csv'\n",
    "with open(path, 'w', encoding = 'utf-8-sig') as f:\n",
    "  pd.DataFrame(HCP_globals_harm).to_csv(f,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZukKGZ1DD6YI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YTAwLer6dCM9"
   },
   "source": [
    "# CR pipeline features : M-ComBat + LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 689,
     "status": "ok",
     "timestamp": 1698432399784,
     "user": {
      "displayName": "inês Sampaio",
      "userId": "15878877488891250619"
     },
     "user_tz": -120
    },
    "id": "eSgAJh3sdCM-",
    "outputId": "b9292675-ee30-4251-93ad-663476d1110c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7]\n",
      "[neuroCombat] Creating design matrix\n",
      "[neuroCombat] Standardizing data across features\n",
      "[neuroCombat] Fitting L/S model and finding priors\n",
      "[neuroCombat] Finding parametric adjustments\n",
      "[neuroCombat] Final adjustment of data\n",
      "[0 1 2 3 4 5 6 7]\n",
      "[neuroCombat] Creating design matrix\n",
      "[neuroCombat] Standardizing data across features\n",
      "[neuroCombat] Fitting L/S model and finding priors\n",
      "[neuroCombat] Finding parametric adjustments\n",
      "[neuroCombat] Final adjustment of data\n",
      "Batchs for this set ['0' '1' '2' '3' '4' '5' '6' '7']\n",
      "Batchs from the training set estimation ['0' '1' '2' '3' '4' '5' '6' '7']\n",
      "M-ComBat option was used during fitting\n",
      "Batches in current set matching the ones from training set estimation [0 1 2 3 4 5 6 7]\n",
      "Transforming tthe reference and other sets used in the combat estimation\n",
      " Reintroducing original data for all data from reference site\n",
      "Batchs for this set ['0' '1' '2' '3' '4' '5' '6' '7']\n",
      "Batchs from the training set estimation ['0' '1' '2' '3' '4' '5' '6' '7']\n",
      "M-ComBat option was used during fitting\n",
      "Batches in current set matching the ones from training set estimation [0 1 2 3 4 5 6 7]\n",
      "Transforming tthe reference and other sets used in the combat estimation\n",
      " Reintroducing original data for all data from reference site\n",
      "SS: Shape of the whole dataset (1472, 170)\n",
      "SS: Selecting only the reference data\n",
      "SS: Shape of reference dataset (1109, 170)\n",
      "SS: Transforming dataset shape (1472, 170)\n",
      "LR: Shape of the whole dataset (1472, 170)\n",
      "LR:Fitting on reference dataset of shape (1109, 170)\n",
      "cortical\n",
      "volumes\n",
      "LR: Transforming dataset shape (1472, 170)\n",
      "SS: Shape of the whole dataset (1472, 170)\n",
      "SS: Selecting only the reference data\n",
      "SS: Shape of reference dataset (1109, 170)\n",
      "SS: Transforming dataset shape (1472, 170)\n",
      "Batchs for this set ['1' '2' '3' '4' '5' '6' '7']\n",
      "Batchs from the training set estimation ['0' '1' '2' '3' '4' '5' '6' '7']\n",
      "M-ComBat option was used during fitting\n",
      "Batches in current set matching the ones from training set estimation [1 2 3 4 5 6 7]\n",
      "Transforming a test set with reference estimations\n",
      "Batchs for this set ['1' '2' '3' '4' '5' '6' '7']\n",
      "Batchs from the training set estimation ['0' '1' '2' '3' '4' '5' '6' '7']\n",
      "M-ComBat option was used during fitting\n",
      "Batches in current set matching the ones from training set estimation [1 2 3 4 5 6 7]\n",
      "Transforming a test set with reference estimations\n",
      "SS: Transforming dataset shape (200, 170)\n",
      "LR: Transforming dataset shape (200, 170)\n",
      "SS: Transforming dataset shape (200, 170)\n"
     ]
    }
   ],
   "source": [
    "# Brain features index-order definition\n",
    "dk40_feat= np.arange(0,68) # 68 cortical thickness\n",
    "cobravgm_feat=np.arange(68,118) # 52 gray matter volumes\n",
    "cobravwm_feat=np.arange(118,170) # 52 white mater volumes\n",
    "vol_feat=np.concatenate((cobravgm_feat,cobravwm_feat))\n",
    "\n",
    "# Create the dictionaries with data + covariates for harmonization\n",
    "\n",
    "HC_dict={'data': HC_data , 'covariates': HC_covars}\n",
    "\n",
    "BD_dict={'data': Strat_data_nothc , 'covariates': strat_covars_nothc}\n",
    "\n",
    "# Brain feature-types to consider separatly and associated bio-covariates\n",
    "feat_of_interest={'cortical': {'id': dk40_feat, 'categorical': ['Gender'],\n",
    "                               'continuous':['Age']},\n",
    "                  'volumes': {'id': vol_feat, 'categorical': ['Gender'],\n",
    "                              'continuous':['Age','TIV']}}\n",
    "\n",
    "# Preprocessing pipelne: Regressing-out confounding biological covariates\n",
    "ref_batch=0 # Is the HCP dataset\n",
    "\n",
    "combat_ref_data=ComBatHarmonization(cv_method=None, ref_batch=ref_batch,\n",
    "                                    regression_fit=1,feat_detail=feat_of_interest,\n",
    "                                    feat_of_no_interest=None)\n",
    "\n",
    "scaler_data_fun_1=StandardScalerDict(ref_batch=ref_batch,std_data=True, std_cov=False,\n",
    "                                     output_dict=True)\n",
    "\n",
    "scaler_data_fun_2=StandardScalerDict(ref_batch=ref_batch,std_data=True, std_cov=False,\n",
    "                                     output_dict=False)\n",
    "\n",
    "biocov_regre=BiocovariatesRegression(ref_batch=ref_batch,cv_method=None,\n",
    "                                     feat_detail=feat_of_interest,\n",
    "                                     output_dict=True)\n",
    "\n",
    "combat_biocovregr_steps = [('Multi-center features Harmonization', combat_ref_data),\n",
    " ('Standardize only data 1', scaler_data_fun_1),('Confounder Regression', biocov_regre),\n",
    "  ('Standardize only data 2', scaler_data_fun_2)]\n",
    "\n",
    "preprocessing_pipeline = Pipeline(combat_biocovregr_steps)\n",
    "\n",
    "# Fit_transform method in training set\n",
    "HC_corr=preprocessing_pipeline.fit_transform(HC_dict)\n",
    "\n",
    "# Transforming Test set\n",
    "BD_corr=preprocessing_pipeline.transform(BD_dict)\n",
    "\n",
    "# Put together test set Data\n",
    "Strat_data_corr=pd.concat([pd.DataFrame(HC_corr[1109:]), pd.DataFrame(BD_corr)], axis=0)\n",
    "Strat_covars=pd.concat([HC_covars.iloc[1109:], strat_covars_nothc], ignore_index=True)\n",
    "\n",
    "HCP_data_corr=pd.DataFrame(HC_corr[:1109])\n",
    "HCP_covars_corr=HC_covars.iloc[:1109].drop(labels=['index'], axis=1).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Igd3F077dCM-"
   },
   "source": [
    "# Save Data in .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K03PAFWddCM-"
   },
   "outputs": [],
   "source": [
    "# Save Data - only harmonized with ref_batch=HCP\n",
    "main_path='****/Corrected_withglobals_modmean/combat_modmean_lr_betas/'\n",
    "\n",
    "path= main_path + 'test_set_YA_data_corr_withglobals.csv'\n",
    "with open(path, 'w', encoding = 'utf-8-sig') as f:\n",
    "  pd.DataFrame(Strat_data_corr).to_csv(f,index=False)\n",
    "\n",
    "path= main_path + 'test_set_YA_covar_corr_withglobals.csv'\n",
    "with open(path, 'w', encoding = 'utf-8-sig') as f:\n",
    "  pd.DataFrame(Strat_covars).to_csv(f,index=False)\n",
    "\n",
    "path= main_path + 'HCP_data_corr.csv'\n",
    "with open(path, 'w', encoding = 'utf-8-sig') as f:\n",
    "  pd.DataFrame(HCP_data_corr).to_csv(f,index=False)\n",
    "\n",
    "# The covariates remain unchanged but we save it in the same folder for convinience\n",
    "path= main_path + 'HCP_covars_corr.csv'\n",
    "with open(path, 'w', encoding = 'utf-8-sig') as f:\n",
    "  pd.DataFrame(HCP_covars_corr).to_csv(f,index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cx2FCUBgdJAv"
   },
   "source": [
    "# CR pipeline: only M-ComBat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 579,
     "status": "ok",
     "timestamp": 1698432431815,
     "user": {
      "displayName": "inês Sampaio",
      "userId": "15878877488891250619"
     },
     "user_tz": -120
    },
    "id": "TQ3kxBxAdJAv",
    "outputId": "2c65bf7e-0cd8-4c88-8b9d-6767256d909a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7]\n",
      "[neuroCombat] Creating design matrix\n",
      "[neuroCombat] Standardizing data across features\n",
      "[neuroCombat] Fitting L/S model and finding priors\n",
      "[neuroCombat] Finding parametric adjustments\n",
      "[neuroCombat] Final adjustment of data\n",
      "[0 1 2 3 4 5 6 7]\n",
      "[neuroCombat] Creating design matrix\n",
      "[neuroCombat] Standardizing data across features\n",
      "[neuroCombat] Fitting L/S model and finding priors\n",
      "[neuroCombat] Finding parametric adjustments\n",
      "[neuroCombat] Final adjustment of data\n",
      "Batchs for this set ['0' '1' '2' '3' '4' '5' '6' '7']\n",
      "Batchs from the training set estimation ['0' '1' '2' '3' '4' '5' '6' '7']\n",
      "M-ComBat option was used during fitting\n",
      "Batches in current set matching the ones from training set estimation [0 1 2 3 4 5 6 7]\n",
      "Transforming tthe reference and other sets used in the combat estimation\n",
      " Reintroducing original data for all data from reference site\n",
      "Batchs for this set ['0' '1' '2' '3' '4' '5' '6' '7']\n",
      "Batchs from the training set estimation ['0' '1' '2' '3' '4' '5' '6' '7']\n",
      "M-ComBat option was used during fitting\n",
      "Batches in current set matching the ones from training set estimation [0 1 2 3 4 5 6 7]\n",
      "Transforming tthe reference and other sets used in the combat estimation\n",
      " Reintroducing original data for all data from reference site\n",
      "Batchs for this set ['1' '2' '3' '4' '5' '6' '7']\n",
      "Batchs from the training set estimation ['0' '1' '2' '3' '4' '5' '6' '7']\n",
      "M-ComBat option was used during fitting\n",
      "Batches in current set matching the ones from training set estimation [1 2 3 4 5 6 7]\n",
      "Transforming a test set with reference estimations\n",
      "Batchs for this set ['1' '2' '3' '4' '5' '6' '7']\n",
      "Batchs from the training set estimation ['0' '1' '2' '3' '4' '5' '6' '7']\n",
      "M-ComBat option was used during fitting\n",
      "Batches in current set matching the ones from training set estimation [1 2 3 4 5 6 7]\n",
      "Transforming a test set with reference estimations\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Brain features index-order definition\n",
    "dk40_feat= np.arange(0,68) # 68 cortical thickness\n",
    "cobravgm_feat=np.arange(68,118) # 52 gray matter volumes\n",
    "cobravwm_feat=np.arange(118,170) # 52 white mater volumes\n",
    "vol_feat=np.concatenate((cobravgm_feat,cobravwm_feat))\n",
    "\n",
    "# Create the dictionaries with data + covariates for harmonization\n",
    "\n",
    "X_normative_dict={'data': HC_data , 'covariates': HC_covars}\n",
    "\n",
    "BD_test_dict={'data': Strat_data_nothc , 'covariates': strat_covars_nothc}\n",
    "\n",
    "# Brain feature-types to consider separatly and associated bio-covariates\n",
    "feat_of_interest={'cortical': {'id': dk40_feat, 'categorical': ['Gender'],\n",
    "                               'continuous':['Age']},\n",
    "                  'volumes': {'id': vol_feat, 'categorical': ['Gender'],\n",
    "                              'continuous':['Age','TIV']}}\n",
    "\n",
    "# Preprocessing pipelne: Regressing-out confounding biological covariates\n",
    "ref_batch=0 # Is the HCP dataset\n",
    "\n",
    "combat_ref_data=ComBatHarmonization(cv_method=None, ref_batch=ref_batch,\n",
    "                                    regression_fit=0,feat_detail=feat_of_interest,\n",
    "                                    feat_of_no_interest=None)\n",
    "\n",
    "# Fit_transform method in training set\n",
    "X_normative_harm=combat_ref_data.fit_transform(X_normative_dict)\n",
    "\n",
    "# Transforming Test set\n",
    "BD_harm=combat_ref_data.transform(BD_test_dict)\n",
    "\n",
    "# Put together test set Data\n",
    "Strat_data_harm=pd.concat([pd.DataFrame(X_normative_harm[1109:]), pd.DataFrame(BD_harm)], axis=0)\n",
    "Strat_covars_harm=pd.concat([HC_covars.iloc[1109:], strat_covars_nothc], ignore_index=True)\n",
    "\n",
    "# Put together HCP data\n",
    "HCP_data_harm=pd.DataFrame(X_normative_harm[:1109])\n",
    "HCP_covars_harm=HC_covars.iloc[:1109].drop(labels=['index'], axis=1).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EKIBC_NTdJAw"
   },
   "source": [
    "# Save onlyharmonize data in .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ESBUC8szdJAw"
   },
   "outputs": [],
   "source": [
    "# Save Data - only harmonized with ref_batch=HCP\n",
    "main_path='****/Corrected_withglobals_modmean/combat_modmean_lr_betas/'\n",
    "\n",
    "path= main_path + 'test_set_YA_data_onlyharm_withglobals.csv'\n",
    "with open(path, 'w', encoding = 'utf-8-sig') as f:\n",
    "  pd.DataFrame(Strat_data_harm).to_csv(f,index=False)\n",
    "\n",
    "path= main_path + 'test_set_YA_covar_onlyharm_withglobals.csv'\n",
    "with open(path, 'w', encoding = 'utf-8-sig') as f:\n",
    "  pd.DataFrame(Strat_covars_harm).to_csv(f,index=False)\n",
    "\n",
    "path= main_path + 'HCP_data_onlyharm.csv'\n",
    "with open(path, 'w', encoding = 'utf-8-sig') as f:\n",
    "  pd.DataFrame(HCP_data_harm).to_csv(f,index=False)\n",
    "\n",
    "# The covariates remain unchanged but we save it in the same folder for convinience\n",
    "path= main_path + 'HCP_covars_onlyharm.csv'\n",
    "with open(path, 'w', encoding = 'utf-8-sig') as f:\n",
    "  pd.DataFrame(HCP_covars_harm).to_csv(f,index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP/xUQbgU36P5F90i63qTWF",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
